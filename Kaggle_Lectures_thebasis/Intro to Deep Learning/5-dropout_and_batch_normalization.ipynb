{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout and Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning dünyası sadece dense layer'lardan ibaret değil. Bir modele ekleyebileceğiniz düzinelerce layer türü var. (Örnek için Keras dokümanlarına göz atabilirsiniz!) Bazıları dense layer'lar gibi neuron'lar arasındaki bağlantıları tanımlarken, diğerleri preprocessing veya başka türde dönüşümler yapabilir.\n",
    "\n",
    "Bu derste, kendi içlerinde neuron içermeyen, ancak bir modele çeşitli şekillerde fayda sağlayabilen iki özel layer türünü öğreneceğiz. Her ikisi de modern mimarilerde yaygın olarak kullanılmaktadır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bunlardan ilki, overfitting'i düzeltmeye yardımcı olabilen \"dropout layer\"dır.\n",
    "\n",
    "Geçen derste, overfitting'in network'ün training data'daki sahte pattern'ları öğrenmesinden kaynaklandığından bahsetmiştik. Bu sahte pattern'ları tanımak için network genellikle çok spesifik weight kombinasyonlarına, bir tür weight \"conspiracy\"sine güvenir. Bu kadar spesifik oldukları için kırılgandırlar: birini kaldırırsanız conspiracy çöker.\n",
    "\n",
    "Dropout'un arkasındaki fikir budur. Bu conspiracy'leri (komplo) bozmak için, eğitimin her adımında bir layer'ın input unit'lerinin bir kısmını rastgele düşürürüz. Bu, network'ün training data'daki sahte pattern'ları öğrenmesini zorlaştırır. Bunun yerine, weight pattern'ları daha sağlam olan geniş, genel pattern'lar aramak zorunda kalır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout'u bir tür network ensemble'ı oluşturma olarak da düşünebilirsiniz. Tahminler artık tek bir büyük network tarafından değil, daha küçük network'lerden oluşan bir komite tarafından yapılır. Komitedeki bireyler genellikle farklı türde hatalar yapar, ancak aynı zamanda doğru olurlar. Bu da komiteyi bir bütün olarak herhangi bir bireyden daha iyi yapar. (Eğer random forest'ların decision tree'lerin bir ensemble'ı olduğu fikrine aşinaysanız, aynı fikirdir.)\n",
    "\n",
    "Eng.\n",
    "\n",
    "You could also think about dropout as creating a kind of ensemble of networks. The predictions will no longer be made by one big network, but instead by a committee of smaller networks. Individuals in the committee tend to make different kinds of mistakes, but be right at the same time, making the committee as a whole better than any individual. (If you're familiar with random forests as an ensemble of decision trees, it's the same idea.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras'ta, dropout rate argümanı rate, kapatılacak input unit'lerinin yüzdesini tanımlar. Dropout layer'ı, dropout'un uygulanmasını istediğiniz layer'dan hemen önce yerleştirin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keras.Sequential([\\n    # ...\\n    layers.Dropout(rate=0.3),\\n    layers.Dense(16),\\n    # ...\\n])'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''keras.Sequential([\n",
    "    # ...\n",
    "    layers.Dropout(rate=0.3),\n",
    "    layers.Dense(16),\n",
    "    # ...\n",
    "])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu örnekte, Dense layer'a giden input'ların %30'u rastgele kapatılacaktır. Dropout genellikle %20 ile %50 arasında ayarlanır, ancak en iyi değer probleme ve modele bağlı olarak değişebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verilerin normalize edilmesi neden önemli? Neural network'lerde, verileri ortak bir ölçeğe getirmek, modelin daha iyi ve dengeli bir şekilde öğrenmesini sağlar. Örneğin, StandardScaler veya MinMaxScaler gibi araçlar, verileri network'e girmeden önce normalleştirir, yani her bir özelliği (feature) belli bir ölçeğe getirir. Bu, özellikle Stokastik Gradyan İnişi (SGD) algoritmasının ağırlıkları daha tutarlı bir şekilde güncellemesi için önemlidir. Eğer veriler çok farklı büyüklüklerde olursa, model dengesiz bir şekilde öğrenebilir.\n",
    "\n",
    "Batch normalization nedir? Verileri network'e girmeden önce normalize etmek iyi bir uygulamadır. Ancak, batch normalization (batchnorm) bunu modelin içinde yapar. Yani model, her bir batch (küçük veri grubu) için verileri kendi ortalaması ve standart sapmasına göre normalize eder. Sonrasında, bu verileri iki tane öğrenilebilir parametre kullanarak yeni bir ölçeğe koyar. Bu da modelin içindeki her aşamada verilerin dengeli kalmasını sağlar.\n",
    "\n",
    "Batch normalization'ın faydaları:\n",
    "\n",
    "Eğitim sürecini hızlandırabilir. Batch normalization içeren modeller genellikle daha az epoch (döngü) ile eğitimi tamamlar.\n",
    "Eğitim sırasında oluşabilecek sorunları çözebilir. Özellikle modelin takıldığı veya öğrenmekte zorlandığı durumlarda batch normalization kullanmak faydalı olabilir.\n",
    "Özetle, batch normalization hem eğitim sürecini optimize eder hem de modelin daha dengeli ve tutarlı bir şekilde öğrenmesine yardımcı olur. Eğer modelini eğitirken yavaşlama veya sorunlarla karşılaşıyorsan, batch normalization eklemeyi düşünebilirsin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adding Batch Normalization**\n",
    "\n",
    "Bir katmandan hemen sonra eklenebilir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.Dense(16, activation='relu'),\n",
    "# layers.BatchNormalization(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "veya katman ve aktivasyon fonksiyonunun arasına eklenebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.Dense(16),\n",
    "# layers.BatchNormalization(),\n",
    "# layers.Activation('relu'),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ve eğer bunu ağınızın ilk katmanı olarak eklerseniz, Sci-Kit Learn'in StandardScaler'ı gibi bir şeyin yerini alarak bir tür uyarlanabilir ön işlemci olarak işlev görebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ÖRNEK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
