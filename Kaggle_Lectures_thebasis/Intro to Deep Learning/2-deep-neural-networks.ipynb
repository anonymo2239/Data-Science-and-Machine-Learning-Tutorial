{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu derste, derin sinir ağlarının ünlü olduğu karmaşık ilişkileri öğrenebilen sinir ağları nasıl oluşturabileceğimizi göreceğiz.\n",
    "\n",
    "Buradaki temel fikir, modülerliktir; bu, basit fonksiyonel birimlerden karmaşık bir ağ inşa etmeyi içerir. Doğrusal bir birimin doğrusal bir fonksiyonu nasıl hesapladığını gördük -- şimdi bu tekil birimleri nasıl birleştirip değiştirebileceğimizi ve daha karmaşık ilişkileri nasıl modelleyebileceğimizi göreceğiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sinir ağları, nöronlarını katmanlar içinde organize ederler. Ortak bir girdi kümesine sahip olan doğrusal birimleri bir araya getirdiğimizde, yoğun bir katman elde ederiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Metin](image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her bir katmanı sinir ağında basit bir işlem gerçekleştiren bir birim olarak düşünebiliriz. Katmanların derin bir yığını aracılığıyla, bir sinir ağı girdilerini giderek daha karmaşık yollarla dönüştürebilir. İyi eğitilmiş bir sinir ağında, her katman bizi çözüme biraz daha yaklaştıran bir dönüşüm gerçekleştirir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktivasyon Fonksiyonu\n",
    "\n",
    "Ancak, aralarında hiçbir şey olmayan iki yoğun katman, tek başına bir yoğun katmandan daha iyi değildir. Yalnızca yoğun katmanlar, bizi doğrusal ve düzlemsel dünyadan çıkaramaz. İhtiyacımız olan şey, doğrusal olmayan bir şeydir. İhtiyacımız olan şey, aktivasyon fonksiyonlarıdır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AKTİVASYON FONKSİYONU OLMAYAN SİNİR AĞLARI, YALNIZCA DOĞRUSAL İLİŞKİLERİ ÖĞRENEBİLİR. Eğrileri fit etmek için aktivasyon fonksiyonlarını kullanmamız gerekecek. Önceden gerekmiyordu çünkü lineer doğrularla uğraşıyorduk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aktivasyon fonksiyonu basitçe, bir katmanın her bir çıktısına uygulanan bazı fonksiyonlardır. En yaygın olanı rektifiye (düzeltici) fonksiyonudur: max(0, x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Metin](image6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rektifiye (düzeltici) fonksiyonunun grafiği, negatif kısmı sıfıra \"düzeltilmiş\" bir çizgidir. Bu fonksiyonu bir nöronun çıktısına uygulamak, veride bir bükülme oluşturur ve bizi basit çizgilerden uzaklaştırır."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineer birime, rectifier fonksiyonunu uyguladığımızda rectified linear unit (ReLU)(doğrultulmuş doğrusal birim) elde ederiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bir doğrusal birime ReLU aktivasyonu uygulamak, çıktının max(0, w⋅x+b) haline gelmesi anlamına gelir. Bunu bir diyagramda şöyle gösterebiliriz:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoğun Katmanları Yığma\n",
    "\n",
    "Artık elimizde doğrusal olmayan bir yapı olduğuna göre, katmanları yığarak nasıl karmaşık veri dönüşümleri elde edebileceğimize bakalım."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "katmanları neden yığıyoruz ki\n",
    "\n",
    "\n",
    "Katmanları yığmamızın amacı, sinir ağının girdileri daha karmaşık ve soyut temsillere dönüştürebilmesini sağlamaktır. Her bir katman, belirli bir veri seti üzerinde farklı bir dönüşüm gerçekleştirir. İlk katmanlar daha basit özellikleri öğrenirken, katmanlar ilerledikçe, ağ daha karmaşık ilişkiler ve desenler öğrenebilir. Böylece, derin katmanlar sayesinde ağ, veriyi basit bir şekilde doğrusal olarak ayıramayacağı durumlarda bile daha karmaşık kararlar alabilir. Yani, katmanları yığarak modelin öğrenme kapasitesini artırır ve daha zengin bir çıktı elde edebiliriz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](image8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yoğun katmanlı bir yığın tam bağlantılı bir ağ oluşturur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output'dan (y) önceki katmanlara \"hidden\" deriz Çünkü direkt olarak çıktılarını göremeyiz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son katmanın (çıktı katmanı) bir doğrusal birim olduğunu (yani, aktivasyon fonksiyonu olmadığını) fark edin. Bu, bu ağın bir regresyon görevine uygun olduğunu gösterir, çünkü belirli bir sayısal değeri tahmin etmeye çalışırız. Diğer görevler (örneğin, sınıflandırma) çıktıda bir aktivasyon fonksiyonu gerektirebilir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Sequential(SIRALI) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aşağıdaki kodda kullanmış olduğumuz Sequential Model, baştan sona liste halindeki katmanları birbirine bağlayacak. İlk katman input u alır. Son katman çıktıyı üretir. Bu yukardaki şekli üretir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alperen Arda\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katmanları ayrı ayrı argümanlar olarak değil, bir liste halinde geçirdiğinizden emin olun, örneğin [katman, katman, katman, ...]. Bir katmana aktivasyon fonksiyonu eklemek için, sadece activation argümanına fonksiyonun adını verin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
